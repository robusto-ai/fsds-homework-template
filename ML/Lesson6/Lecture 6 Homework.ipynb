{"cells":[{"cell_type":"markdown","metadata":{"id":"4gLKAuy94wN7"},"source":["# Lecture 6: K-Nearest Neighbor and Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"8vmdF6MV4wN_"},"source":["## Exercise 1: Implement the Euclidean Distance Function\n","\n","**Task**: Write a Python function euclidean_distance(point1, point2) that calculates the Euclidean distance between two points represented as lists of coordinates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7Io-XrQ4wOB"},"outputs":[],"source":["# Solution\n","import math\n","\n","def euclidean_distance(point1, point2):\n","    if len(point1) != len(point2):\n","        raise ValueError(\"Points must have the same dimensions.\")\n","    if not point1 and not point2:\n","        return 0\n","    return math.sqrt(sum((x - y) ** 2 for x, y in zip(point1, point2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4Jy3nmk4wOF"},"outputs":[],"source":["# Unit tests\n","def test_euclidean_distance():\n","    # Test for points in 2D space\n","    assert abs(euclidean_distance([0, 0], [3, 4]) - 5.0) < 1e-6\n","    # Test for points in 3D space\n","    assert abs(euclidean_distance([1, 2, 3], [4, 5, 6]) - 5.196) < 1e-3\n","    # Test for same points\n","    assert euclidean_distance([1, 1], [1, 1]) == 0\n","    # Test for error on mismatched dimensions\n","    try:\n","        euclidean_distance([1, 2], [1])\n","        assert False, \"Expected ValueError for mismatched dimensions\"\n","    except ValueError:\n","        pass\n","    # Test for one point being empty\n","    try:\n","        euclidean_distance([], [1, 2, 3])\n","        assert False, \"Expected ValueError for empty point\"\n","    except ValueError:\n","        pass\n","    # Test for both points being empty\n","    assert euclidean_distance([], []) == 0, \"Expected distance to be 0 for empty points\""]},{"cell_type":"markdown","metadata":{"id":"TqyuIsGQ4wOH"},"source":["## Exercise 2: Implement the Naive Bayes Classifier for Binary Data\n","\n","**Task**: Implement a basic Naive Bayes classifier ```naive_bayes_predict(priors, likelihoods, features)``` for binary classification. The function should:\n","\n","1. Take the prior probabilities, likelihood probabilities for each feature, and the observed feature values as input.\n","\n","2. Predict the class (0 or 1) based on Bayes' Theorem.\n","\n","**Input and Output Format**:\n","\n","1. Input Format:\n","- priors: A list containing the prior probabilities for class 0 and class 1.\n","\n","    For example:\n","    ```python\n","    priors = [0.5, 0.5]  # Equal priors for both classes\n","    ```\n","\n","\n","- likelihoods: A nested list where each sublist contains the likelihood probabilities for each feature in the respective class.\n","\n","    For example:\n","\n","    ```python\n","    likelihoods = [\n","        [0.8, 0.6],  # Likelihoods for class 0\n","        [0.4, 0.7]   # Likelihoods for class 1\n","    ]\n","    ```\n","\n","- features: A list of observed feature values (1 or 0).\n","\n","    For example:\n","\n","    ```python\n","    features = [1, 0]  # Observed feature values\n","    ```\n","\n","2. Output Format:\n","- The predicted class (0 or 1).\n","\n","    For example:\n","\n","    ```python\n","    predicted_class = 0\n","    ```\n","\n","**Examples**:\n","\n","1. Example 1:\n","- Input:\n","\n","    ```python\n","    priors = [0.5, 0.5]\n","    likelihoods = [\n","        [0.8, 0.6],  # Class 0\n","        [0.4, 0.7]   # Class 1\n","    ]\n","    features = [1, 0]\n","    ```\n","\n","- Calculation:\n","\n","    - For class 0:\n","\n","    $$P(class=0∣features)=P(class=0)⋅P(feature_{1}=1∣class=0)⋅P(feature_{2}=0∣class=0)=0.5⋅0.8⋅(1−0.6)=0.16$$\n","\n","    - For class 1:\n","\n","    $$P(class=1∣features)=P(class=1)⋅P(feature_{1}=1∣class=1)⋅P(feature_{2}=0∣class=1)=0.5⋅0.4⋅(1−0.7)=0.06$$\n","\n","- Output:\n","\n","    ```python\n","    predicted_class = 0  # Class 0 has the higher posterior probability\n","    ```\n","\n","2. Example 2:\n","\n","- Input:\n","\n","    ```python\n","    priors = [0.3, 0.7]\n","    likelihoods = [\n","        [0.9, 0.2],  # Class 0\n","        [0.3, 0.8]   # Class 1\n","    ]\n","    features = [1, 1]\n","    ```\n","\n","- Calculation:\n","\n","    - For class 0:\n","\n","    $$P(class=0∣features)=P(class=0)⋅P(feature_{1}=1∣class=0)⋅P(feature_{2}=1∣class=0)=0.3⋅0.9⋅0.2=0.054$$\n","\n","    - For class 1:\n","\n","    $$P(class=1∣features)=P(class=1)⋅P(feature_{1}=1∣class=1)⋅P(feature_{2}=1∣class=1)=0.7⋅0.3⋅0.8=0.168$$\n","\n","- Output:\n","\n","    ```python\n","    predicted_class = 1  # Class 1 has the higher posterior probability\n","    ```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NL5G9YRl4wOK"},"outputs":[],"source":["def naive_bayes_predict(priors, likelihoods, features):\n","    if len(likelihoods[0]) != len(features):\n","        raise ValueError(\"Number of features must match the likelihoods.\")\n","\n","    posterior_0 = priors[0]\n","    posterior_1 = priors[1]\n","\n","    for i, feature in enumerate(features):\n","        posterior_0 *= likelihoods[0][i] if feature == 1 else (1 - likelihoods[0][i])\n","        posterior_1 *= likelihoods[1][i] if feature == 1 else (1 - likelihoods[1][i])\n","\n","    return 0 if posterior_0 > posterior_1 else 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40TsyGBg4wOM"},"outputs":[],"source":["def test_naive_bayes_predict():\n","    priors = [0.5, 0.5]\n","    likelihoods = [[0.8, 0.6], [0.4, 0.7]]\n","\n","    # Test for features favoring class 0\n","    assert naive_bayes_predict(priors, likelihoods, [1, 0]) == 0\n","    # Test for features favoring class 1\n","    assert naive_bayes_predict(priors, likelihoods, [0, 1]) == 1\n","    # Test for equal likelihoods favoring prior\n","    assert naive_bayes_predict(priors, likelihoods, [1, 1]) == 0\n","    # Test for mismatched feature size\n","    try:\n","        naive_bayes_predict(priors, likelihoods, [1])\n","        assert False, \"Expected ValueError for mismatched feature size\"\n","    except ValueError:\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"7OkBKycv4wON"},"source":["## Exercise 3: Implement K-Nearest Neighbors Classifier\n","\n","**Task**: Implement a function knn_predict(data, labels, query, k) that:\n","\n","1. Takes a dataset, corresponding labels, a query point, and the number of neighbors ($k$) as input.\n","\n","2. Predicts the label for the query point based on the majority class of its $k$-nearest neighbors.\n","\n","**Input and Output Format**:\n","\n","1. Input Format:\n","\n","- data: A list of points in the dataset, where each point is represented as a list of features.\n","\n","    For example:\n","\n","    ```python\n","    data = [[1, 2], [3, 4], [5, 6]]\n","    ```\n","\n","- labels: A list of integers representing the class labels for the corresponding points in the dataset.\n","\n","    For example:\n","\n","    ```python\n","    labels = [0, 1, 1]\n","    ```\n","\n","- query: A single point represented as a list of features.\n","\n","    For example:\n","\n","    ```python\n","    query = [4, 5]\n","    ```\n","\n","- k: An integer representing the number of nearest neighbors to consider.\n","\n","    For example:\n","\n","    ```python\n","    k = 2\n","    ```\n","\n","2. Output Format:\n","- The predicted class label for the query point (an integer).\n","\n","    For example:\n","\n","    ```python\n","    predicted_label = 1\n","    ```\n","\n","**Examples**:\n","\n","- Input:\n","\n","    ```python\n","    data = [[1, 1], [2, 2], [3, 3], [6, 6]]\n","    labels = [0, 0, 1, 1]\n","    query = [4, 4]\n","    k = 3\n","    ```\n","\n","- Steps:\n","\n","    - Calculate distances from the query point [4, 4] to all points in data:\n","\n","        - Distance to $\\left[1, 1\\right]$: $\\sqrt{\\left( 4-1 \\right)^{2} + \\left( 4-1 \\right)^{2}}=4.24$\n","\n","        - Distance to $\\left[2, 2\\right]$: $\\sqrt{\\left( 4-2 \\right)^{2} + \\left( 4-2 \\right)^{2}}=2.83$\n","\n","        - Distance to $\\left[3, 3\\right]$: $\\sqrt{\\left( 4-3 \\right)^{2} + \\left( 4-3 \\right)^{2}}=1.41$\n","\n","        - Distance to $\\left[6, 6\\right]$: $\\sqrt{\\left( 4-6 \\right)^{2} + \\left( 4-6 \\right)^{2}}=2.83$\n","\n","    - Sort distances and select the 3 nearest neighbors:\n","\n","        - Nearest points: $\\left[3, 3\\right], \\left[2, 2\\right], \\left[6, 6\\right]$\n","\n","        - Labels: $\\left[1, 0, 1\\right]$\n","\n","    - Perform majority vote:\n","\n","        - Class 0: 1 vote\n","\n","        - Class 1: 2 votes\n","\n","    - Predicted label: 1\n","\n","- Output:\n","\n","    ```python\n","    predicted_label = 1\n","    ```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhiIdSYi4wOP"},"outputs":[],"source":["def knn_predict(data, labels, query, k):\n","    distances = [(euclidean_distance(query, point), label) for point, label in zip(data, labels)]\n","    distances.sort(key=lambda x: x[0])\n","    k_nearest = distances[:k]\n","\n","    class_votes = {}\n","    for _, label in k_nearest:\n","        class_votes[label] = class_votes.get(label, 0) + 1\n","\n","    return max(class_votes, key=class_votes.get)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xvjq3rov4wOQ"},"outputs":[],"source":["def test_knn_predict():\n","    data = [[1, 1], [2, 2], [3, 3], [6, 6]]\n","    labels = [0, 0, 1, 1]\n","\n","    # Test with k=1\n","    assert knn_predict(data, labels, [2, 2], 1) == 0\n","    # Test with k=3\n","    assert knn_predict(data, labels, [4, 4], 3) == 1\n","    # Test with tie-breaking in k=2\n","    assert knn_predict(data, labels, [3, 3], 2) == 1\n","    # Test for query identical to a data point\n","    assert knn_predict(data, labels, [6, 6], 1) == 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xO3Qvjtq4wOR"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}