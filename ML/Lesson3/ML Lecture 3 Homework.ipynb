{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbQxq0FiAMBg"
      },
      "source": [
        "# Lecture 3: Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zycFihQoAMBi"
      },
      "source": [
        "## Exercise 1: Implement the Sigmoid Function\n",
        "\n",
        "**Task**: Write a Python function to calculate the sigmoid function for a given input $z$. The sigmoid function is defined as:\n",
        "â€‹\n",
        "$$\\sigma\\left( z \\right) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "**Example**:\n",
        "\n",
        "```bash\n",
        ">>> sigmoid(0)\n",
        "0.5\n",
        ">>> sigmoid(2)\n",
        "0.8807970779778823\n",
        ">>> sigmoid(-2)\n",
        "0.11920292202211755\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "213TSG6_AMBi"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + math.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fScxwzLvAMBj"
      },
      "outputs": [],
      "source": [
        "def test_sigmoid():\n",
        "    assert abs(sigmoid(0) - 0.5) < 1e-6\n",
        "    assert abs(sigmoid(2) - 0.8807970779778823) < 1e-6\n",
        "    assert abs(sigmoid(-2) - 0.11920292202211755) < 1e-6\n",
        "\n",
        "test_sigmoid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCslD25uAMBj"
      },
      "source": [
        "## Exercise 2: Compute Binary Cross-Entropy Loss\n",
        "\n",
        "**Task**: Write a Python function to compute the binary cross-entropy loss for a given set of predictions and true labels. Use the formula:\n",
        "\n",
        "$$L=-\\frac{1}{N}\\sum_{i=1}^{N}\\left( y_{i}log\\left( p_{i} \\right) + \\left( 1-y_{i} \\right)log\\left( 1-p_{i} \\right) \\right)$$\n",
        "\n",
        "- Input:\n",
        "\n",
        "    - y_true: A list of true labels (0 or 1).\n",
        "\n",
        "    - y_pred: A list of predicted probabilities (values between 0 and 1).\n",
        "\n",
        "- Output: A single float representing the binary cross-entropy loss.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "```bash\n",
        ">>> binary_cross_entropy([1, 0, 1], [0.9, 0.1, 0.8])\n",
        "0.164252033486018\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMO7qI6IAMBj"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    loss = 0\n",
        "    for yt, yp in zip(y_true, y_pred):\n",
        "        loss += yt * math.log(yp) + (1 - yt) * math.log(1 - yp)\n",
        "    return -loss / n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuixXHCKAMBk"
      },
      "outputs": [],
      "source": [
        "def test_binary_cross_entropy():\n",
        "    assert abs(binary_cross_entropy([1, 0], [0.9, 0.1]) - 0.10536051565782628) < 1e-6\n",
        "    assert abs(binary_cross_entropy([1, 1], [0.8, 0.7]) - 0.24116205681688812) < 1e-6\n",
        "    assert abs(binary_cross_entropy([0, 0], [0.3, 0.4]) - 0.4581453659370776) < 1e-6\n",
        "\n",
        "test_binary_cross_entropy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM_LkE9oAMBk"
      },
      "source": [
        "## Exercise 3: Compute Confusion Matrix\n",
        "\n",
        "**Task**: Write a Python function to compute the confusion matrix for binary classification. The function should return the counts of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
        "\n",
        "- Input:\n",
        "\n",
        "    - y_true: A list of true labels (0 or 1).\n",
        "\n",
        "    - y_pred: A list of predicted labels (0 or 1).\n",
        "\n",
        "- Output: A dictionary with keys TP, TN, FP, FN.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "```python\n",
        ">>> confusion_matrix([1, 0, 1, 0], [1, 0, 0, 1])\n",
        "{'TP': 1, 'TN': 1, 'FP': 1, 'FN': 1}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRIeV8ifAMBk"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix(y_true, y_pred):\n",
        "    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
        "    tn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 0)\n",
        "    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
        "    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
        "    return {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXHKENXCAMBk"
      },
      "outputs": [],
      "source": [
        "def test_confusion_matrix():\n",
        "    assert confusion_matrix([1, 0, 1, 0], [1, 0, 0, 1]) == {'TP': 1, 'TN': 1, 'FP': 1, 'FN': 1}\n",
        "    assert confusion_matrix([1, 1, 1], [1, 1, 1]) == {'TP': 3, 'TN': 0, 'FP': 0, 'FN': 0}\n",
        "    assert confusion_matrix([0, 0, 0], [1, 1, 1]) == {'TP': 0, 'TN': 0, 'FP': 3, 'FN': 0}\n",
        "\n",
        "test_confusion_matrix()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}